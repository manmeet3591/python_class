// This file was automatically generated by 'make' from file 'hyb_spat_to_SH.gen.c'.
// To modify it, please consider modifying hyb_spat_to_SH.gen.c
/*
 * Copyright (c) 2010-2015 Centre National de la Recherche Scientifique.
 * written by Nathanael Schaeffer (CNRS, ISTerre, Grenoble, France).
 * 
 * nathanael.schaeffer@ujf-grenoble.fr
 * 
 * This software is governed by the CeCILL license under French law and
 * abiding by the rules of distribution of free software. You can use,
 * modify and/or redistribute the software under the terms of the CeCILL
 * license as circulated by CEA, CNRS and INRIA at the following URL
 * "http://www.cecill.info".
 * 
 * The fact that you are presently reading this means that you have had
 * knowledge of the CeCILL license and that you accept its terms.
 * 
 */

//////////////////////////////////////////////////
  #ifdef SHT_AXISYM
/// The spatial field is assumed to be \b axisymmetric (spatial size NLAT), and only the m=0 harmonics are written to output.
  #endif

/// Truncation and spatial discretization are defined by \ref shtns_create and \ref shtns_set_grid_*
/// \param[in] shtns = a configuration created by \ref shtns_create with a grid set by shtns_set_grid_*
/// \param[in] Vr = spatial scalar field : double array.
/// \param[in] Vt, Vp = spatial (theta, phi) vector components : double arrays.
/// \param[out] Qlm = spherical harmonics coefficients :
/// complex double arrays of size shtns->nlm.
/// \param[out] Slm,Tlm = spherical harmonics coefficients of \b Spheroidal and \b Toroidal scalars :
/// complex double arrays of size shtns->nlm.
  #ifdef SHT_VAR_LTR
/// \param[in] llim = specify maximum degree of spherical harmonic. llim must be at most shtns->lmax, and all spherical harmonic degree higher than llim are set to zero. 
  #endif

	static void GEN3(spat_to_SHqst_,ID_NME,SUFFIX)(shtns_cfg shtns, double *Vr, double *Vt, double *Vp, cplx *Qlm, cplx *Slm, cplx *Tlm, long int llim) {
  #ifndef SHT_GRAD
  #else
	static void GEN3(spat_to_SHsph_,ID_NME,SUFFIX)(shtns_cfg shtns, double *Vt, cplx *Slm, long int llim) {
	static void GEN3(spat_to_SHtor_,ID_NME,SUFFIX)(shtns_cfg shtns, double *Vp, cplx *Tlm, long int llim) {
  #endif

	double *zl;
	double *dzl0;
	struct DtDp *dzl;
	long int i,i0, ni,l;
  #ifndef SHT_AXISYM
	unsigned im, imlim;
	cplx *BrF;		// contains the Fourier transformed data
	cplx *BtF, *BpF;	// contains the Fourier transformed data
	v2d reo[2*NLAT_2];	// symmetric (even) and anti-symmetric (odd) parts, interleaved.
	v2d tpeo[4*NLAT_2];	// theta and phi even and odd parts
	#define reo0 ((double*)reo)
	#define tpeo0 ((double*)tpeo)
	#define te0(i)	tpeo0[4*(i)]
	#define to0(i)	tpeo0[4*(i)+1]
	#define pe0(i)	tpeo0[4*(i)+2]
	#define po0(i)	tpeo0[4*(i)+3]
	#define vteo0(i)	tpeo[2*(i)]
	#define vpeo0(i)	tpeo[2*(i)+1]
  #else
	double reo0[2*NLAT_2+2] SSE;	// symmetric (even) and anti-symmetric (odd) parts, interleaved.
	double teo0[2*NLAT_2+2] SSE;
	double peo0[2*NLAT_2+2] SSE;
	#define te0(i)	teo0[2*(i)]
	#define to0(i)	teo0[2*(i)+1]
	#define pe0(i)	peo0[2*(i)]
	#define po0(i)	peo0[2*(i)+1]
	#define vteo0(i)	((s2d*) teo0)[i]
	#define vpeo0(i)	((s2d*) peo0)[i]
  #endif

	#define ZL(i) vdup(dzl[i].p)

// defines how to access even and odd parts of data
	#define re(i)	reo[2*(i)]
	#define ro(i)	reo[2*(i)+1]
	#define te(i)	tpeo[4*(i)]
	#define to(i)	tpeo[4*(i)+1]
	#define pe(i)	tpeo[4*(i)+2]
	#define po(i)	tpeo[4*(i)+3]
	#define re0(i)	reo0[2*(i)+1]
	#define ro0(i)	reo0[2*(i)]

  #ifndef SHT_AXISYM
	BrF = (cplx *) Vr;
	BtF = (cplx *) Vt;
	BpF = (cplx *) Vp;
	if (shtns->ncplx_fft >= 0) {
	    if (shtns->ncplx_fft > 0) {		// alloc memory for the FFT
	    	BrF = VMALLOC( 3* shtns->ncplx_fft * sizeof(cplx) );
	    	BtF = BrF + shtns->ncplx_fft;		BpF = BtF + shtns->ncplx_fft;
	    }
	    fftw_execute_dft_r2c(shtns->fft,Vr, BrF);
	    fftw_execute_dft_r2c(shtns->fft,Vt, BtF);
	    fftw_execute_dft_r2c(shtns->fft,Vp, BpF);
	}
	imlim = MTR;
	#ifdef SHT_VAR_LTR
		if (imlim*MRES > (unsigned) llim) imlim = ((unsigned) llim)/MRES;		// 32bit mul and div should be faster
	#endif
  #endif

	ni = NLAT_2;	// copy NLAT_2 to a local variable for faster access (inner loop limit)
	//	im = 0;		// dzl.p = 0.0 : and evrything is REAL
  #ifndef SHT_NO_DCT
	double* st_1 = shtns->st_1;
	#ifndef SHT_AXISYM
		#define BR0	((double *)reo)
		#define BT0	((double *)tpeo)
		#define BP0	((double *)tpeo + NLAT)
		i=0;  do {	// we assume NPHI>1 (else SHT_AXISYM should be defined).
			double sin_1 = st_1[i];
			((double *)BtF)[i*2] *= sin_1; 	((double *)BpF)[i*2] *= sin_1;
		} while (++i<NLAT);
		fftw_execute_r2r(shtns->dct_m0,(double *) BrF, BR0);		// DCT out-of-place.
		fftw_execute_r2r(shtns->dct_m0,(double *) BtF, BT0);		// DCT out-of-place.
		fftw_execute_r2r(shtns->dct_m0,(double *) BpF, BP0);		// DCT out-of-place.
	#else
		#define BR0	reo0
		#define BT0	teo0
		#define BP0	peo0
		i=0;	do {
		#ifdef _GCC_VEC_
			s2d sin_1 = ((s2d *)st_1)[i];
			((s2d*) Vt)[i] *= sin_1;
		 	((s2d*) Vp)[i] *= sin_1;
		#else
			double sin_1 = st_1[2*i];		double sin_2 = st_1[2*i+1];
			Vt[2*i] *= sin_1;		Vt[2*i+1] *= sin_2;
			Vp[2*i] *= sin_1;		Vp[2*i+1] *= sin_2;
		#endif
		} while (++i<ni);
		fftw_execute_r2r(shtns->dct_m0,Vr, BR0);	// DCT out-of-place.
		fftw_execute_r2r(shtns->dct_m0,Vt, BT0);	// DCT out-of-place.
		fftw_execute_r2r(shtns->dct_m0,Vp, BP0);	// DCT out-of-place.
	#endif
		l=0;
		long int klim = shtns->klim;
		#ifdef SHT_VAR_LTR
			i = (llim * SHT_NL_ORDER) + 2;		// sum truncation
			if (i < klim) klim = i;
		#endif
		v2d* Ql = (v2d*) Qlm;
		v2d* Sl = (v2d*) Slm;
		v2d* Tl = (v2d*) Tlm;
		zl = shtns->zlm_dct0;
		dzl0 = shtns->dzlm_dct0;
	#ifndef _GCC_VEC_
		cplx s1 = 0.0;		// l=0 : Sl = 0
		cplx t1 = 0.0;		// l=0 : Tl = 0
	#else
		v2d s = vdup(0.0);		// l=0 : Sl = 0
		v2d t = vdup(0.0);		// l=0 : Tl = 0
	#endif
	#ifdef SHT_VAR_LTR
		while(l < llim) {
	#else
		do {		// l < LMAX
	#endif
			i=l;	// l < klim
	  #ifndef _GCC_VEC_
			Sl[l] = s1;
			Tl[l] = t1;
			cplx q0 = 0.0;	cplx q1 = 0.0;
			cplx s0 = 0.0;	s1 = 0.0;
			cplx t0 = 0.0;	t1 = 0.0;
			do {
				q0 += BR0[i]   * zl[i];
				q1 += BR0[i+1] * zl[i+1];
				s0 += BT0[i]   * dzl0[i];
				t0 -= BP0[i]   * dzl0[i];
				s1 += BT0[i+1] * dzl0[i+1];
				t1 -= BP0[i+1] * dzl0[i+1];
				i+=2;
			} while(i<klim);
			Ql[l] = q0;		Ql[l+1] = q1;
			Sl[l+1] = s0;
			Tl[l+1] = t0;
	  #else
			Sl[l] = vhi_to_cplx(s);
			Tl[l] = vhi_to_cplx(t);
			s = vdup(0.0);
			t = vdup(0.0);
			s2d q = vdup(0.0);
			i >>= 1;	// i = i/2
			do {
				q += ((s2d*) zl)[i] * ((s2d*) BR0)[i];
				s += ((s2d*) dzl0)[i] * ((s2d*) BT0)[i];
				t -= ((s2d*) dzl0)[i] * ((s2d*) BP0)[i];
				++i;
			} while(2*i < klim);
			Ql[l]   = vlo_to_cplx(q);		Ql[l+1] = vhi_to_cplx(q);
			Sl[l+1] = vlo_to_cplx(s);
			Tl[l+1] = vlo_to_cplx(t);
	  #endif
			l+=2;
			zl += (shtns->klim - l);
			dzl0 += (shtns->klim -l);
	#ifndef SHT_VAR_LTR
		} while(l<llim);
	#else
		}
	#endif
		if (l == llim) {
	#ifndef _GCC_VEC_
			Sl[l] = s1;
			Tl[l] = t1;
	#else
			((v2d*) Sl)[l] = vhi_to_cplx(s);
			((v2d*) Tl)[l] = vhi_to_cplx(t);
	#endif
			cplx q0 = 0.0;
			i=l;	// l < klim
			do {
				q0 += BR0[i] * zl[i];
				i+=2;
			} while(i<klim);
			((cplx *) Ql)[l] = q0;
			++l;
		}
	#undef BR0
	#undef BT0
	#undef BP0
	#ifdef SHT_VAR_LTR
		while( l<=LMAX ) {
			Ql[l] = vdup(0.0);
			Sl[l] = vdup(0.0);
			Tl[l] = vdup(0.0);
			++l;
		}
	#endif
  #else		// ifndef SHT_NO_DCT
		i=0;
		zl = shtns->zlm[0];
		// stride of source data : we assume NPHI>1 (else SHT_AXISYM should be defined).
	#ifndef SHT_AXISYM
		#define BR0(i) ((double*)BrF)[(i)*2]
		#define BT0(i) ((double*)BtF)[(i)*2]
		#define BP0(i) ((double*)BpF)[(i)*2]
	#else
		#define BR0(i) Vr[i]
		#define BT0(i) Vt[i]
		#define BP0(i) Vp[i]
	#endif
	#if _GCC_VEC_ && __SSE3__
		s2d r0v = vdup(0.0);
		do {	// compute symmetric and antisymmetric parts.
			s2d a = vdup(BR0(i));		s2d b = vdup(BR0(NLAT-1-i));
			a = subadd(a,b);
			((s2d*) reo0)[i] = a;		// assume odd is first, then even.
			r0v += vdup(zl[i]) * a;	// even part is used.
			s2d c = vdup(BT0(i));		s2d d = vdup(BT0(NLAT-1-i));
			c = subadd(c,d);		vteo0(i) = vxchg(c);
			s2d e = vdup(BP0(i));		s2d f = vdup(BP0(NLAT-1-i));
			e = subadd(e,f);		vpeo0(i) = vxchg(e);
			++i;
		} while(i<ni);
		((v2d*)Qlm)[0] = vhi_to_cplx(r0v);
	#else
		double r0 = 0.0;
		do {	// compute symmetric and antisymmetric parts.
			double a = BR0(i);		double b = BR0(NLAT-1-i);
			ro0(i) = (a-b);		re0(i) = (a+b);
			r0 += zl[i] * (a+b);
			double c = BT0(i);		double d = BT0(NLAT-1-i);
			te0(i) = (c+d);		to0(i) = (c-d);
			double e = BP0(i);		double f = BP0(NLAT-1-i);
			pe0(i) = (e+f);		po0(i) = (e-f);
		} while(++i<ni);
		Qlm[0] = r0;
	#endif
		#undef BR0
		#undef BT0
		#undef BP0
		zl += ni + (ni&1);		// SSE alignement
		l=1;			// l=0 is zero for the vector transform.
		v2d* Ql = (v2d*) Qlm;		// virtual pointer for l=0 and im
		v2d* Sl = (v2d*) Slm;		// virtual pointer for l=0 and im
		v2d* Tl = (v2d*) Tlm;		// virtual pointer for l=0 and im
		dzl0 = (double *) shtns->dzlm[0];		// only theta derivative (d/dphi = 0 for m=0)
		Sl[0] = vdup(0.0);	// l=0 is zero for the vector transform.
		Tl[0] = vdup(0.0);	// l=0 is zero for the vector transform.
	#ifdef SHT_VAR_LTR
		while (l<llim) {		// ops : NLAT/2 * (2*(LMAX-m+1) + 4) : almost twice as fast.
	#else
		do {
	#endif
			i=0;
  #ifndef _GCC_VEC_
			double q0 = 0.0;
			double q1 = 0.0;
			double s0 = 0.0;	double s1 = 0.0;
			double t0 = 0.0;	double t1 = 0.0;
			do {
				q0 += zl[0] * ro0(i);	// Qlm[LiM(l,im)] += zlm[im][(l-m)*NLAT/2 + i] * fp[i];
				q1 += zl[1] * re0(i);	// Qlm[LiM(l+1,im)] += zlm[im][(l+1-m)*NLAT/2 + i] * fm[i];
				s0 += dzl0[0] * te0(i);
				t0 -= dzl0[0] * pe0(i);
				s1 += dzl0[1] * to0(i);
				t1 -= dzl0[1] * po0(i);
				zl +=2;
				dzl0 +=2;
			} while(++i < ni);
			Ql[l] = q0;
			Ql[l+1] = q1;
			Sl[l] = s0;		Sl[l+1] = s1;
			Tl[l] = t0;		Tl[l+1] = t1;
  #else
			s2d s = vdup(0.0);
			s2d t = vdup(0.0);
			s2d q = vdup(0.0);
			do {
				s += ((s2d*) dzl0)[i] * vteo0(i);
				t -= ((s2d*) dzl0)[i] * vpeo0(i);
				q += ((s2d*) zl)[i] * ((s2d*) reo0)[i];
				++i;
			} while(i < ni);
			zl += 2*ni;
			dzl0 += 2*ni;
			Ql[l] = vlo_to_cplx(q);		Ql[l+1] = vhi_to_cplx(q);
			Sl[l] = vlo_to_cplx(s);		Sl[l+1] = vhi_to_cplx(s);
			Tl[l] = vlo_to_cplx(t);		Tl[l+1] = vhi_to_cplx(t);
  #endif
			l+=2;
	#ifndef SHT_VAR_LTR
		} while (l<llim);
	#else
		}
	#endif
		if (l==llim) {
			long int lstride=1;
	  #ifdef SHT_VAR_LTR
			if (l != LMAX) lstride=2;
	  #endif
			double q0 = 0.0;
			double s0 = 0.0;
			double t0 = 0.0;
			i=0;	do {
				q0 += zl[0] * ro0(i);		// Qlm[LiM(l,im)] += zlm[im][(l-m)*NLAT/2 + i] * fp[i];
				s0 += dzl0[0] * te0(i);
				t0 -= dzl0[0] * pe0(i);
				zl += lstride;
				dzl0 += lstride;
				++i;
			} while(i<ni);
			((cplx *)Ql)[l] = q0;
			((cplx *)Sl)[l] = s0;
			((cplx *)Tl)[l] = t0;
	  #ifdef SHT_VAR_LTR
	  		++l;
		}
	    while( l<=LMAX ) {
			Ql[l] = vdup(0.0);
			Sl[l] = vdup(0.0);
			Tl[l] = vdup(0.0);
			++l;
      #endif
		}
  #endif		// ifndef SHT_NO_DCT
  #ifndef SHT_AXISYM
	for (im=1;im<=imlim;++im) {
		BrF += NLAT;
		BtF += NLAT;	BpF += NLAT;
		i0 = shtns->tm[im];
 		i=i0;
		do {	// compute symmetric and antisymmetric parts.
			s2d sin = vdup(shtns->st[i]);
			v2d q0 = ((v2d *)BrF)[i];	v2d q1 = ((v2d *)BrF)[NLAT-1-i];		re(i) = (q0+q1)*sin;	ro(i) = (q0-q1)*sin;
			v2d t0 = ((v2d *)BtF)[i];	v2d t1 = ((v2d *)BtF)[NLAT-1-i];		te(i) = t0+t1;	to(i) = t0-t1;
			v2d s0 = ((v2d *)BpF)[i];	v2d s1 = ((v2d *)BpF)[NLAT-1-i];		pe(i) = s0+s1;	po(i) = s0-s1;
 		} while (++i<ni);
		l = LiM(shtns, 0,im);
		v2d* Ql = (v2d*) &Qlm[l];	// virtual pointer for l=0 and im
		v2d* Sl = (v2d*) &Slm[l];		v2d* Tl = (v2d*) &Tlm[l];
		l=im*MRES;
		double m_1 = 1.0/l;
		zl = shtns->zlm[im];
		dzl = shtns->dzlm[im];
		while (l<llim) {		// ops : NLAT/2 * (2*(LMAX-m+1) + 4) : almost twice as fast.
			v2d q0 = vdup(0.0);
			v2d q1 = vdup(0.0);
			v2d s0 = vdup(0.0);	v2d t1 = vdup(0.0);		v2d s0i = vdup(0.0);	v2d t1i = vdup(0.0);
			v2d t0 = vdup(0.0);	v2d s1 = vdup(0.0);		v2d t0i = vdup(0.0);	v2d s1i = vdup(0.0);
			i=i0;	do {		// tm[im] : polar optimization
				s0  += vdup(dzl[0].t) *to(i);		// ref: these E. Dormy p 72.
				t0  -= vdup(dzl[0].t) *po(i);
				s1  += vdup(dzl[1].t) *te(i);
				t1  -= vdup(dzl[1].t) *pe(i);
				s0i -= vdup(dzl[0].p) *pe(i);
				t0i -= vdup(dzl[0].p) *te(i);
				s1i -= vdup(dzl[1].p) *po(i);
				t1i -= vdup(dzl[1].p) *to(i);
				q0  += re(i) * ZL(0);		// Qlm[LiM(l,im)] += zlm[im][(l-m)*NLAT/2 + i] * fp[i];
				q1  += ro(i) * ZL(1);		// Qlm[LiM(l+1,im)] += zlm[im][(l+1-m)*NLAT/2 + i] * fm[i];
				zl +=2;
				dzl +=2;
			} while (++i < ni);
			q0 *= vdup((l*(l+1))*m_1);
			q1 *= vdup(((l+1)*(l+2))*m_1);
			Sl[l] = addi(s0,s0i);	Tl[l+1] = addi(t1,t1i);
			Tl[l] = addi(t0,t0i);	Sl[l+1] = addi(s1,s1i);
			Ql[l] = q0;
			Ql[l+1] = q1;
			l+=2;
		}
		if (l==llim) {
			long int lstride=1;
	  #ifdef SHT_VAR_LTR
			if (l != LMAX) lstride=2;
	  #endif
			v2d q0 = vdup(0.0);	// Qlm[LiM(l,im)] = 0.0;
			v2d s0 = vdup(0.0);	v2d s0i = vdup(0.0);
			v2d t0 = vdup(0.0);	v2d t0i = vdup(0.0);
			i=i0;	do {		// tm[im] : polar optimization
				s0  += vdup(dzl[0].t) *to(i);
				t0  -= vdup(dzl[0].t) *po(i);
				s0i -= vdup(dzl[0].p) *pe(i);
				t0i -= vdup(dzl[0].p) *te(i);
				q0  += re(i) * ZL(0);		// Qlm[LiM(l,im)] += zlm[im][(l-m)*NLAT/2 + i] * fp[i];
				zl  += lstride;
				dzl += lstride;
			} while(++i<ni);
			q0 *= vdup((l*(l+1))*m_1);
			Sl[l] = addi(s0,s0i);
			Tl[l] = addi(t0,t0i);
			Ql[l] = q0;
	  #ifdef SHT_VAR_LTR
	  		++l;
		}
	    while( l<=LMAX ) {
			Ql[l] = vdup(0.0);
			Sl[l] = vdup(0.0);	Tl[l] = vdup(0.0);
			++l;
      #endif
		}
	}
	#ifdef SHT_VAR_LTR
	if (imlim < MMAX) {
		im = imlim+1;
		l = LiM(shtns, im*MRES, im);
		do {
			((v2d*)Qlm)[l] = vdup(0.0);
			((v2d*)Slm)[l] = vdup(0.0);		((v2d*)Tlm)[l] = vdup(0.0);
		} while(++l < shtns->nlm);
	}
	#endif

  	if (shtns->ncplx_fft > 0) {		// free memory
	    VFREE(BrF - NLAT*imlim);
	}
  #endif

	#undef ZL
	#undef re
	#undef ro
	#undef te
	#undef to
	#undef pe
	#undef po
	#undef re0
	#undef ro0
	#undef te0
	#undef to0
	#undef pe0
	#undef po0
	#undef reo0
	#undef teo0
	#undef peo0
	#undef reo0
	#undef tpeo0
	#undef vteo0
	#undef vpeo0
  }
